{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea80d16",
   "metadata": {},
   "source": [
    "# Data!\n",
    "\n",
    "This notebook focuses on actual data that we input to a neural network. We focus on:\n",
    "- Accquiring the data\n",
    "- Preprocessing data\n",
    "- How to iterate over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca33440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64a81",
   "metadata": {},
   "source": [
    "Torchvision is a collection of data/datasets that we can use to train our model on. It has a bunch of vision data. \n",
    "Next we need to define our training and testing dataset.\n",
    "\n",
    "## MNIST Data\n",
    "Next we use MNIST dataset. The data is not in the form of a tensor, so we need to transform it into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdf5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a11b705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a570901",
   "metadata": {},
   "source": [
    "# Generalizing Data\n",
    "## Batch Size\n",
    "The reason why we batch data is because our data is going to be so big that we might not be able to fit it on a GPU. The second reason is that if you pass the entire data through the model at once, the machine might learn the stuff specific to the data in addition to the 'general' stuff. What we do is that we pass the data in batches so it only optimizes for the 'general' stuff.\n",
    "\n",
    "Batch size is also a trial and error thing. There is a sweet spot wich is usually between 8 and 64, regardless of how big the memory is.\n",
    "\n",
    "You want your batch size to be as big as possible because that impacts how quickly you're going to train through your data.\n",
    "\n",
    "## Shuffle\n",
    "We need to shuffle a non sequential data because, taking an example, if we feed in all the 0s at once, machine would think that everything is 0, then we'll feed 1, and the machine will think everything is 1.. and so on.\n",
    "\n",
    "#### Both the steps are carried out here to generalize as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddf674",
   "metadata": {},
   "source": [
    "Next, lets see how the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e07fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 9, 6, 3, 3, 8, 0, 0, 5, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b8387",
   "metadata": {},
   "source": [
    "The above print is representing a batch, we have 10 examples of hand written digits and the corresponding pixels or tensors.\n",
    "\n",
    "Next, we can show the image using Matplotlib. But for that we need to reshape the tensor to a $28\\times 28$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7405ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159f6c1",
   "metadata": {},
   "source": [
    "The 1 needs to be dealt with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03969971",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "# data is a list of tensors, where x will be the first batch matrix and y will be the actual answer/number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ddc362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4849f35",
   "metadata": {},
   "source": [
    "As seen in the print(data), the first example is that of a 0, we reshaped the tensor and used matplotlib to visualize the pixel image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85696fe6",
   "metadata": {},
   "source": [
    "# Balancing the data\n",
    "It is important to balance the training dataset, it means that there should be adequate examples of all the instances. If there is just 60% 3 in our dataset, the machine will learn to always predict a 3 to minimize the loss and get stuck there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cacdbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da397d3c",
   "metadata": {},
   "source": [
    "Now, finding the % of all the data instances, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e229bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]*100/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bd2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
